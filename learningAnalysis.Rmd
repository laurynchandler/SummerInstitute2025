---
title: "learningAnalysis_07_24"
output: html_document
date: "2025-07-24"
---



```{r load packages, echo=FALSE, include=FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)
library(performance)
library(patchwork)
library(emmeans)
library(corrplot)
library(Hmisc)
library(broom)
library(corrr)
library(reshape2)
```


```{r load data, message=FALSE, warning=FALSE}
# learning data
df_learn <- read.csv('data/learning.csv')
df_learn <- df_learn %>%
  filter(trial < 91) %>%
  mutate(scaledTrial = as.numeric(scale(trial)),
         scaledLogTrial = as.numeric(scale(log(trial)))) %>%
  group_by(subID) %>%
  mutate(scaledImgLockedRT = as.numeric(scale(imgLockedRT))) %>%
  ungroup()

# estimates data
df_est <- read.csv('data/estimates.csv')
df_est <- df_est %>%
  group_by(subID) %>%
  mutate(zCueConf = as.numeric(scale(cueConfidence)),
         cueCorr = cor(trueCue, subjectiveCue),
         cueDiff = subjectiveCue - trueCue) %>%
  ungroup()

# inference data
df_inf <- read.csv('data/inference_test_tidy.csv') %>% select(-X)
df_inf <- df_inf %>%
  group_by(subID) %>%
  mutate(zconf = scale(confidence),
         zconfRT = scale(confRT))
```

# slope of regression lines during learning: all subjects

## predicting z-scored image-locked RT using `trueCue * scaledLogTrial`, controlling for response finger using `imageIdx` as a regressor
```{r}
# fit model
m <- lm(scaledImgLockedRT ~ scaledLogTrial * trueCue + imageIdx, data = df_learn)
# plot trends
emmip(m, trueCue ~ scaledLogTrial, CIs=T, at=list(trueCue = unique(df_learn$trueCue),
                                                    scaledLogTrial = range(df_learn$scaledLogTrial))) + 
  geom_hline(yintercept=mean(df_learn$scaledImgLockedRT,na.rm=T)) + labs(y='scaledimgLockedRT', title = 'fitted trends: scaledimgLockedRT ~ scaledLogTrial * trueCue')
# view summary
summary(m) 
```
- significant main effect of trueCue: people are faster for more predictive trueCues
- potentially trending interaction between trial & trueCue: people get (marginally) faster for more predictive trueCues over the course of learning

## compare to just `scaledTrial` as the predictor
```{r}
# fit model
m <- lm(scaledImgLockedRT ~ scaledTrial * trueCue + imageIdx, data = df_learn)
# plot trends
emmip(m, trueCue ~ scaledTrial, CIs=T, at=list(trueCue = unique(df_learn$trueCue),
                                                    scaledTrial = range(df_learn$scaledTrial))) + 
  geom_hline(yintercept=mean(df_learn$scaledImgLockedRT,na.rm=T)) + labs(y='scaledimgLockedRT', title = 'fitted trends: scaledimgLockedRT ~ scaledTrial * trueCue')
# view summary
summary(m) 
```

# aaron question: which one should we use?

---

# empirical plots: scaledImgLockedRT by scaledLogTrial
```{r, include=F}
df_learn %>%
  mutate(trueCue = factor(trueCue))%>%
  ggplot(aes(x=scaledLogTrial, y=scaledImgLockedRT, color=trueCue, fill=trueCue)) +
  theme_bw() + facet_wrap(~subID, nrow=4) + geom_hline(yintercept = 0, color='gray30') +
  geom_point(size=0.5, alpha=0.7) + stat_smooth(method='lm', linewidth=0.75)
```

# fit regression model for each subject, extract simple slopes for trueCue
```{r, echo=F}
trueCue_coefs <- list()
trueCue_trends <- list()

for(sub_id in unique(df_learn$subID)) {
  df_sub <- df_learn %>% filter(subID == sub_id)
  model_sub <- lm(scaledImgLockedRT ~ scaledLogTrial * trueCue + imageIdx, data = df_sub)

  # Get tidy results with coefficients, t-values, and p-values
  tidy_true <- tidy(model_sub)
  # Add subject ID
  tidy_true$subID <- sub_id
  
  # get the emtrends
  trends <- emtrends(model_sub, ~ trueCue, var='scaledLogTrial', at=list(trueCue=unique(df_sub$trueCue))) %>% 
    as.data.frame() %>% mutate(subID = sub_id)

  # Add to lists
  trueCue_coefs[[length(trueCue_coefs) + 1]] <- tidy_true
  trueCue_trends[[length(trueCue_trends) + 1]] <- trends
}

# Combine all results
df_trueCue_coefs <- bind_rows(trueCue_coefs)
df_trueCue_trends <- bind_rows(trueCue_trends)

# Reorder columns to put subID first
# df_trueCue_coefs <- df_trueCue_coefs[, c("subID", "term", "estimate", "std.error", "statistic", "p.value")]
```
 
 
# relationships between slope of learning RTs & explicit report metrics
## cueCorr: accuracy of cue ordering
```{r}
# make dataframe with just the interaction coefficients
df_trueCue_interaction_coefs <- df_trueCue_coefs %>% filter(term == 'scaledLogTrial:trueCue') 

# plot
df_est %>% select(cueCorr, subID) %>% unique() %>%
  left_join(., df_trueCue_interaction_coefs, by='subID') %>%
  ggplot(aes(x=estimate, y=cueCorr)) + theme_bw() + geom_point() +
  stat_smooth(method='lm') + labs(x='learning RT slope', title = 'predicting cueCorr with slope of RTs during learning')

# fit linear model
df_est %>% select(cueCorr, subID) %>% unique() %>%
  left_join(., df_trueCue_interaction_coefs, by='subID') %>%
  lm(cueCorr ~ estimate, .) %>% summary()
```

## cueDiff: signed difference between reported - true
```{r}

df_cueDiffs <- df_est %>% group_by(subID, trueCue) %>% summarise(meanDiff = mean(cueDiff, na.rm=T))

# make a plot
df_cueDiffs %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  mutate(trueCue = factor(trueCue, levels=c(0.5, 0.65, 0.8))) %>%
  ggplot(., aes(x=scaledLogTrial.trend, y=meanDiff, color=trueCue, fill=trueCue)) + facet_wrap(~trueCue) + geom_hline(yintercept=0) +
  theme_bw() + geom_point() + stat_smooth(method = 'lm') + labs(y = 'average cueDiff', x = 'learning RT slope', title='predicting cueDiff with learning RT slope & trueCue')

# fit a linear model
df_cueDiffs %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  lm(meanDiff ~ scaledLogTrial.trend*trueCue, .) %>% summary()
```
## subjectiveCue: explicitly reported cue probability
```{r}
# plot
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  mutate(trueCue = factor(trueCue, levels=c(0.5, 0.65, 0.8))) %>%
  ggplot(., aes(x=scaledLogTrial.trend, y=subjectiveCue, color=trueCue, fill=trueCue)) + facet_wrap(~trueCue) + 
  theme_bw() + geom_point() + stat_smooth(method = 'lm') + labs(y = 'subjectiveCue', x = 'learning RT slope', title='predicting subjectiveCue using learningRT slope * trueCue')

# fit linear model
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  lm(subjectiveCue ~ scaledLogTrial.trend*trueCue, .) %>% summary()

# compute correlation
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  select(scaledLogTrial.trend, trueCue, subjectiveCue) %>%
  group_by(trueCue) %>%
  summarise(linear_corr = cor(scaledLogTrial.trend, subjectiveCue, method='pearson'),
            nonlinear_corr = cor(scaledLogTrial.trend, subjectiveCue, method='spearman'))
```

## cueConfidence: confidence in subjectiveCue report
```{r}
# plot
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  mutate(trueCue = factor(trueCue, levels=c(0.5, 0.65, 0.8))) %>%
  ggplot(., aes(x=scaledLogTrial.trend, y=zCueConf, color=trueCue, fill=trueCue)) + geom_hline(yintercept = 0) +
  facet_wrap(~trueCue) + 
  theme_bw() + geom_point() + stat_smooth(method = 'lm') + labs(y = 'zCueConfidence', x = 'learning RT slope', title = 'predicting cueConfidence using learningRT slope * trueCue')

# fit linear model
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  lm(zCueConf ~ scaledLogTrial.trend*trueCue, .) %>% summary()

# compute correlation
df_est %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  select(scaledLogTrial.trend, trueCue, zCueConf) %>%
  group_by(trueCue) %>%
  summarise(linear_corr = cor(scaledLogTrial.trend, zCueConf, method='pearson', use='na.or.complete'),
            nonlinear_corr = cor(scaledLogTrial.trend, zCueConf, method='spearman', use='na.or.complete'))
```

---

# effects of learning RT slopes on behavior during inference

## effects on RTs independent of accuracy
```{r}

# make plot
df_inf %>% 
  group_by(subID, congCue, trueCue, trueCongruence) %>%
  summarise(meanRT = mean(zlogRT, na.rm=T)) %>%
  left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  mutate(trueCue = factor(trueCue, levels=c(0.5, 0.65, 0.8))) %>%
  ggplot(., aes(x=scaledLogTrial.trend, y=meanRT, color=factor(congCue), fill=factor(congCue))) + geom_hline(yintercept = 0) +
  facet_grid(~congCue) + 
  theme_bw() + geom_point() + stat_smooth(method = 'lm') + labs(y = 'RTs during inference (decision making)', x = 'learning RT slope')

# fit & plot linear model
# df_inf %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
#   lm(zlogRT ~ scaledLogTrial.trend*congCue, .) %>% #summary()
#   emmip(., ~ scaledLogTrial.trend | congCue, var='congCue', CIs=T,
#         at=list(scaledLogTrial.trend=range(df_trueCue_trends$scaledLogTrial.trend),
#                                                                     congCue = c(0.2, 0.5, 0.8)))
# # emtrends for linear model
# df_inf %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
#   lm(zlogRT ~ scaledLogTrial.trend*congCue, .) %>%
#   emtrends(eff ~ scaledLogTrial.trend, var = 'congCue', 
#            at=list(scaledLogTrial.trend=range(df_trueCue_trends$scaledLogTrial.trend),
#                    congCue=c(0.8)))
```
- x-axis: slope of regression line for RTs during learning --> implicit measure of statistical learning
- y-axis: each subject's average RT during _inference_ --> 0 represents the mean across all participants in that condition
- panels: prediction on that trial, ranging from strongly incongruent (congCue==0.2) to strongly congruent(congCue==0.8). neutral cue (makes no real prediction) in the middle (congCue == 0.5)

interpretation (panel by panel):
- on strong incongruent trials: subjects who got faster at responding during learning (x < 0) made _slower_ responses during decision making.
- on weak incongruent trials: there does not appear to be a strong/meaningful relationship between learning RTs & decision RTs.
- on "neutral" trials: subjects who were faster at responding to the cue during learning (x < 0) were significantly slower to make responses *when this cue was presented* during decision making
- on strong congruent trials: subjects who got faster at responding during learning (x < 0) made _faster_ responses, whereas subjects who got slower at responding during learning made _slower_ responses during decision making

## inference RTs split up by accuracy
```{r}
df_inf %>% 
  mutate(accuracyFactor = factor(accuracy, levels=c(1,0), labels=c('correct', 'incorrect'))) %>%
  group_by(subID, congCue, trueCue, trueCongruence, accuracyFactor) %>%
  summarise(meanRT = mean(zlogRT, na.rm=T)) %>%
  left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  mutate(trueCue = factor(trueCue, levels=c(0.5, 0.65, 0.8))) %>%
  ggplot(., aes(x=scaledLogTrial.trend, y=meanRT, color=factor(congCue), fill=factor(congCue))) + geom_hline(yintercept = 0) +
  facet_grid(~congCue) + 
  theme_bw() + geom_point(aes(shape=accuracyFactor)) + stat_smooth(aes(linetype=accuracyFactor), method = 'lm') + labs(y = 'RTs during inference (decision making)', x = 'learning RT slope')
```

they appear to also be present at some levels of congCue here! specifically:
- there is a strong interaction with accuracy for strong congruent trials
- there is no effect of accuracy on the previously-observed trend for true neutral trials


## TO DO: how accuracy, scaledLogTrial.trend, and congCue interact to shape zconfidence ratings

## TO DO: how accuracy, scaledLogTrial.trend, and congCue interact to shape zconfidence reaction times

---

# SLIDING WINDOW ANALYSIS
### start by setting up dataframe & making some simple plots
```{r}
# compute counter variables
df_learn_new <- df_learn %>%
  group_by(subID) %>%
  mutate(cue1_counter1 = cumsum(cueIdx==1 & imageIdx==1),
         cue1_counter2 = cumsum(cueIdx==1 & imageIdx==2),
         cue2_counter1 = cumsum(cueIdx==2 & imageIdx==1),
         cue2_counter2 = cumsum(cueIdx==2 & imageIdx==2),
         cue3_counter1 = cumsum(cueIdx==3 & imageIdx==1),
         cue3_counter2 = cumsum(cueIdx==3 & imageIdx==2))

# make an example plot
df_learn_new %>% filter(subID == 78) %>%
  ggplot(aes(x=trial)) +
  geom_line(aes(y=cue1_counter1), color='orange') + geom_line(aes(y=cue1_counter2), color='orange', linetype='dashed') +
  geom_line(aes(y=cue2_counter1), color='green') + geom_line(aes(y=cue2_counter2), color='green', linetype='dashed') +
  geom_line(aes(y=cue3_counter1), color='purple') + geom_line(aes(y=cue3_counter2), color='purple', linetype='dashed') + theme_bw()
```

```{r}
# compute number of matches between cue & image observed in each window of trials
window_size <- 5
window_bins <- seq(1, max(df_learn$trial), by=window_size)

df_learn_summary <- df_learn_new %>%
  group_by(subID) %>% 
  mutate(window_bin = ceiling(trial / window_size)) %>%
  group_by(subID, cueIdx, window_bin) %>%
  summarise(nMatch_cue1 = max(cue1_counter1),
            nMismatch_cue1 = max(cue1_counter2),
            nMatch_cue2 = max(cue2_counter2),
            nMismatch_cue2 = max(cue2_counter1),
            img1_cue3 = max(cue3_counter1),
            img3_cue3 = max(cue3_counter2))
```

## first type of sliding window analysis: how does volatility early in learning affect slopes of RTs during learning?
```{r}




```

```{r}
# fit & plot linear model
df_inf %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  lm(zlogRT ~ scaledLogTrial.trend*congCue, .) %>% #summary()
  emmip(., ~ scaledLogTrial.trend | congCue, var='congCue', CIs=T,
        at=list(scaledLogTrial.trend=range(df_trueCue_trends$scaledLogTrial.trend),
                                                                    congCue = c(0.2, 0.5, 0.8)))
# emtrends for linear model
df_inf %>% left_join(., df_trueCue_trends, by=c('subID', 'trueCue')) %>%
  lm(zlogRT ~ scaledLogTrial.trend*congCue, .) %>%
  emtrends(eff ~ scaledLogTrial.trend, var = 'congCue', 
           at=list(scaledLogTrial.trend=range(df_trueCue_trends$scaledLogTrial.trend),
                   congCue=c(0.8)))

```

## individual correlations with different outcome variables of interest
```{r}
coef_trueCue <- df_trueCue_stats %>%
  select(subID, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate)

coef_cueIdx <- df_cueIdx_stats %>%
  select(subID, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate)

trueCue_full <- inner_join(coef_trueCue, df_est, by = "subID")
cueIdx_full  <- inner_join(coef_cueIdx,  df_est, by = "subID")

cor_test_matrix <- function(full_df, outcome_vars, exclude = "subID") {
  coef_vars <- setdiff(names(full_df), c(exclude, outcome_vars))
  
  #numeric columns
  full_df[coef_vars] <- lapply(full_df[coef_vars], function(x) as.numeric(as.character(x)))
  full_df[outcome_vars] <- lapply(full_df[outcome_vars], function(x) as.numeric(as.character(x)))
  
  cor_matrix <- matrix(NA, nrow = length(coef_vars), ncol = length(outcome_vars),
                       dimnames = list(coef_vars, outcome_vars))
  p_matrix <- cor_matrix
  
  for (coef in coef_vars) {
    for (outcome in outcome_vars) {
      x <- full_df[[coef]]
      y <- full_df[[outcome]]
      valid <- complete.cases(x, y)
      if (sum(valid) >= 3) {  
        test <- cor.test(x[valid], y[valid])
        cor_matrix[coef, outcome] <- test$estimate
        p_matrix[coef, outcome] <- test$p.value
      }
    }
  }
  
  list(correlation = cor_matrix, p.value = p_matrix)
}

outcome_vars <- c("cueDiff", "cueCorr", "cueConfidence", "subjectiveCue")

trueCue_corrs <- cor_test_matrix(trueCue_full, outcome_vars)
cueIdx_corrs  <- cor_test_matrix(cueIdx_full,  outcome_vars)

trueCue_corrs$correlation     # Correlations: trueCue model
trueCue_corrs$p.value         # P-values

cueIdx_corrs$correlation      # Correlations: cueIdx model
cueIdx_corrs$p.value          # P-values

# Save correlation matrices
# write.csv(trueCue_corrs$correlation, "trueCue_correlations_matrix.csv", row.names = TRUE)
# write.csv(cueIdx_corrs$correlation,  "cueIdx_correlations_matrix.csv",  row.names = TRUE)

# Save p-value matrices
# write.csv(trueCue_corrs$p.value, "trueCue_pvalues_matrix.csv", row.names = TRUE)
# write.csv(cueIdx_corrs$p.value,  "cueIdx_pvalues_matrix.csv",  row.names = TRUE)

```

# aggregate datasets
merging datasets by subject ( thought it would be interesting to compare variables in different stages of testing) 
```{r}
df_est_agg <- df_est %>%
  group_by(subID) %>%
  summarise(
    subjectiveCue = mean(subjectiveCue, na.rm = TRUE),
    cueConfidence = mean(cueConfidence, na.rm = TRUE),
    cueCorr = first(cueCorr), 
    cueDiff = mean(cueDiff, na.rm = TRUE),
    .groups = 'drop'
  )

#df_inf aggregate
df_inf_agg <- df_inf %>%
  group_by(subID) %>%
  summarise(
    inference_zRT = mean(zlogRT, na.rm = TRUE),
    confidence_rating = mean(confidence, na.rm = TRUE),
    confidence_zRT = mean(zconfRTs, na.rm = TRUE),
    .groups = 'drop'
  )

combined_data <- df_est_agg %>%
  full_join(df_inf_agg, by = "subID")

```


## correlation martix
```{r}
# Select variables
outcome_vars <- combined_data %>%
  select(subjectiveCue, cueConfidence, cueDiff, cueCorr, 
         inference_zRT, confidence_rating, confidence_zRT) 

# Correlation matrix
cor_matrix <- cor(outcome_vars, use = "complete.obs")

# re order variables
desired_order <- c("subjectiveCue", "cueDiff", "cueCorr", "cueConfidence","confidence_rating", "confidence_zRT","inference_zRT")

# Reorder correlation matrix
cor_matrix_reordered <- cor_matrix[desired_order, desired_order]

print(cor_matrix_reordered)
#compute p vals
p_matrix <- outcome_vars %>% 
  as.matrix() %>%
  rcorr()
```

## correlation map
```{r}
# Convert correlation matrix to long format
cor_melted <- data.frame(
  Var1 = rep(rownames(cor_matrix_reordered), ncol(cor_matrix_reordered)),
  Var2 = rep(colnames(cor_matrix_reordered), each = nrow(cor_matrix_reordered)),
  value = as.vector(cor_matrix_reordered)
)

# factor levels to order variables
cor_melted$Var1 <- factor(cor_melted$Var1, levels = desired_order)
cor_melted$Var2 <- factor(cor_melted$Var2, levels = rev(desired_order))


# Create heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "", y = "") 
```